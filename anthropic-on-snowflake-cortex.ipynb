{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "xzhf5e2aa2eigto2c25r",
   "authorId": "4922386765467",
   "authorName": "SJINNA672",
   "authorEmail": "sjinna@buffalo.edu",
   "sessionId": "ea437bfd-d910-4a40-97c0-b838227d86b6",
   "lastEditTime": 1745344471346
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df2d13f5-e982-4f75-b5a0-41a7f0befc50",
   "metadata": {
    "collapsed": false,
    "name": "INTRO_MD",
    "resultHeight": 298
   },
   "source": "# ❄️ Anthropic on Snowflake Cortex\n\nBuilding an intelligent question-answering system using Anthropic's Claude and Snowflake's AI capabilities.\n\nThis notebook demonstrates how to build an end-to-end application that:\n1. Processes PDF documents using Cortex Process Docouments\n2. Creates Cortex Search Service to do keyword and vector searches\n3. Implements a chat interface using Snowflake's Cortex and Anthropic's Claude in Streamlit"
  },
  {
   "cell_type": "markdown",
   "id": "292396c3-271e-4760-949d-a018ae2ecaae",
   "metadata": {
    "collapsed": false,
    "name": "SETUP_ENVIRONMENT_MD",
    "resultHeight": 172
   },
   "source": "## Setting Up Your Environment \n\nFirst, we'll import the required packages and set up our Snowflake session. The notebook uses several key packages:\n- `streamlit`: For creating the interactive chat interface\n- `snowflake-ml-python`: For Snowflake Cortex for embeddings and LLM capabilities\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "IMPORT_PACKAGES",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.cortex import complete, EmbedText768\n",
    "from snowflake.snowpark.types import VectorType, FloatType\n",
    "from snowflake.core.table import Table, TableColumn\n",
    "from snowflake.core import CreateMode, Root\n",
    "from snowflake.snowpark.functions import cast, col\n",
    "\n",
    "\n",
    "session = get_active_session()\n",
    "current_warehouse = session.get_current_warehouse()\n",
    "database_name = session.get_current_database()\n",
    "schema_name = session.get_current_schema()\n",
    "role_name = session.get_current_role()\n",
    "service_name = 'document_search_service'\n",
    "root = Root(session)\n",
    "database = root.databases[database_name]\n",
    "schema = database.schemas[schema_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf55b1e-27f2-4c48-b2f9-2c28a5fd6f26",
   "metadata": {
    "collapsed": false,
    "name": "SETUP_STAGE_VARIABLES_MD",
    "resultHeight": 102
   },
   "source": "## Setting Up Stage Variables \n\nWe'll define our stage name and retrieve the list of files to process. This stage should contain the PDF documents we want to analyze."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ba83db-cebb-4135-b42e-cd914d04979c",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "VARIABLES",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "\nstage_name = \"@ANTHROPIC_RAG.ANTHROPIC_RAG.DOCUMENTS\"\nfiles = session.sql(f\"LIST {stage_name}\").collect()\n"
  },
  {
   "cell_type": "markdown",
   "id": "57f1c904-ac79-4687-abbc-ff58476cbfdb",
   "metadata": {
    "collapsed": false,
    "name": "DOCUMENT_PROCESSING_MD",
    "resultHeight": 102
   },
   "source": "## Document Processing Functions \n\nWe'll create functions to extract text from PDF files\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb862d46-00a2-4c0f-b2f2-f2aa9fd4b932",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "DOCUMENT_PROCESSING",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "def process(file_name: str):\n",
    "    query = \"\"\"\n",
    "        SELECT TO_VARCHAR(\n",
    "            SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n",
    "                ?,\n",
    "                ?,\n",
    "                {'mode': 'OCR'}):content\n",
    "        ) AS OCR;\n",
    "    \"\"\"\n",
    "\n",
    "    resp = session.sql(query, params=[stage_name, file_name]).collect()\n",
    "    text = resp[0]['OCR']\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'TEXT' : [text],\n",
    "        'FILE_NAME': file_name\n",
    "    })\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf82656-1bcb-45ea-9922-431013d3b5eb",
   "metadata": {
    "collapsed": false,
    "name": "CREATE_EMBEDDINGS_MD",
    "resultHeight": 172
   },
   "source": [
    "## Processing Documents\n",
    "\n",
    "Now we'll:\n",
    "1. Process all documents in our stage\n",
    "2. Store the results in our table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca313e-5d8a-4eac-9566-7ab1ca5cc381",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "PROCESS_FILES",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Extract file names and process files\n",
    "file_names = [file['name'].split('/')[1] for file in files]\n",
    "\n",
    "# Download and process files into a DataFrame\n",
    "final_dataframe = pd.concat([\n",
    "    process(file_name)\n",
    "    for file_name in file_names\n",
    "], ignore_index=True)\n",
    "\n",
    "snowpark_df = session.create_dataframe(final_dataframe).select(\n",
    "    col(\"file_name\"),\n",
    "    col(\"text\")\n",
    ")\n",
    "\n",
    "# Write the transformed data directly to the target table\n",
    "snowpark_df.write.mode(\"overwrite\").save_as_table(\"docs_text_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbf882f-d1b9-4ad8-8bd4-431a30583542",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## Create Cortex Search Service \n\n### Key Components Explained\n\n#### Required Parameters\n\n- `ON`: Specifies the column containing the text to be indexed  \n- `ATTRIBUTES`: Additional columns to include in search results (e.g., file\\_name)  \n- `WAREHOUSE`: Compute warehouse for processing the embeddings  \n- `TARGET_LAG`: Maximum allowed lag for index updates  \n- `EMBEDDING_MODEL`: Model used to generate text embeddings  \n- Source query: The SELECT statement defining the data to index\n\n#### Configuration Options\n\n1. Target Lag Settings:  \n     \n   - Shorter lag times mean more frequent updates  \n   - Common values: '1 hour', '1 day', '1 week'  \n   - Balance freshness needs with compute costs\n\n   \n\n2. Embedding Model Options:  \n     \n   - 'snowflake-arctic-embed-l-v2.0': Latest Snowflake embedding model  \n   - Optimized for English language content  \n   - 384-dimensional embeddings\n\n   \n\n3. Warehouse Considerations:  \n     \n   - Choose size based on data volume  \n   - Consider compute costs vs update frequency  \n   - Monitor warehouse utilization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe7945-d6a7-4309-a331-a67b1123238d",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "CREATE_CORTEX_SEARCH_SERVICE",
    "resultHeight": 112
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE CORTEX SEARCH SERVICE {{service_name}}\n",
    "  ON text\n",
    "  ATTRIBUTES file_name\n",
    "  WAREHOUSE = {{current_warehouse}}\n",
    "  TARGET_LAG = '1 day'\n",
    "  EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\n",
    "  AS (\n",
    "    SELECT\n",
    "        text,\n",
    "        file_name\n",
    "    FROM docs_text_table\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca153ba-92e0-4cd2-8e0c-343d781aead3",
   "metadata": {
    "collapsed": false,
    "name": "BUILD_CHAT_INTERFACE_MD",
    "resultHeight": 371
   },
   "source": "## Building the Chat Interface\n\nFinally, we'll create our chat interface that uses:\n- Utilizes the Cortex Search Service for finding relevant context\n- Chat history management for conversation continuity\n- Anthropic's Claude model for generating responses\n- Streamlit for the user interface\n\nKey parameters:\n- `num_results`: Number of context results provided (default: 3)\n- `model_name`: Language model used (default: \"claude-3-5-sonnet\")\n- `history_length`: Chat history length (default: 5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc57aa3-082f-4cc4-a7b0-e01c56ee6d90",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "BUILD_CHAT_INTERFACE",
    "resultHeight": 3622
   },
   "outputs": [],
   "source": [
    "num_results = 8  # Number of results\n",
    "model_name = \"claude-3-5-sonnet\"  # The model we are using\n",
    "history_length = 5 # Number of chat messages in history\n",
    "\n",
    "def init_messages():\n",
    "    \"\"\"\n",
    "    Initialize the session state for chat messages. If the session state indicates that the\n",
    "    conversation should be cleared or if the \"messages\" key is not in the session state,\n",
    "    initialize it as an empty list.\n",
    "    \"\"\"\n",
    "    if st.session_state.clear_conversation or \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "        st.session_state.suggestions = []\n",
    "        st.session_state.active_suggestion = None\n",
    "        \n",
    "\n",
    "def init_config_options():\n",
    "    \"\"\"\n",
    "    Initialize the chat interface configuration and display existing chat history.\n",
    "    Provides a button to clear conversation history and maintains chat state.\n",
    "    \"\"\"\n",
    "\n",
    "    st.session_state.num_chat_messages = history_length\n",
    "    st.button(\"Clear conversation\", key=\"clear_conversation\")\n",
    "    \n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "\n",
    "    # Display chat messages from history on app rerun\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "def get_chat_history():\n",
    "    \"\"\"\n",
    "    Retrieve the chat history from the session state limited to the number of messages\n",
    "\n",
    "    Returns:\n",
    "        list: The list of chat messages from the session state.\n",
    "    \"\"\"\n",
    "    start_index = max(\n",
    "        0, len(st.session_state.messages) - st.session_state.num_chat_messages\n",
    "    )\n",
    "    return st.session_state.messages[start_index : len(st.session_state.messages) - 1]\n",
    "\n",
    "def make_chat_history_summary(chat_history, question):\n",
    "    \"\"\"\n",
    "    Generate a summary of the chat history combined with the current question to extend the query\n",
    "    context. Use the language model to generate this summary.\n",
    "\n",
    "    Args:\n",
    "        chat_history (str): The chat history to include in the summary.\n",
    "        question (str): The current user question to extend with the chat history.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated summary of the chat history and question.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        Given the following conversation history and new question, generate a detailed query that incorporates relevant context from the chat history. The query should be written in natural, conversational language and include any important details, preferences, or constraints mentioned previously.\n",
    "\n",
    "        <chat_history>\n",
    "        {chat_history}\n",
    "        </chat_history>\n",
    "        \n",
    "        <question>\n",
    "        {question}\n",
    "        </question>\n",
    "        \n",
    "        Please generate a single, comprehensive query that combines the above information. The query should be self-contained and allow for a complete response without requiring additional context.\n",
    "    \"\"\"\n",
    "\n",
    "    summary = complete(model_name, prompt)\n",
    "\n",
    "    return summary\n",
    "\n",
    "def cortex_search(my_question):\n",
    "    search_service = (root\n",
    "      .databases[database_name]\n",
    "      .schemas[schema_name]\n",
    "      .cortex_search_services[service_name]\n",
    "    )\n",
    "\n",
    "    resp = search_service.search(\n",
    "      query=my_question,\n",
    "      columns=[\"text\", \"file_name\"],\n",
    "      limit=num_results\n",
    "    )\n",
    "\n",
    "    results = json.loads(resp.to_json())[\"results\"]\n",
    "    prompt_context = \"\"\n",
    "\n",
    "    # Building the context from the search results\n",
    "    for result in results:\n",
    "        prompt_context += result[\"text\"]\n",
    "    prompt_context = prompt_context.replace(\"'\", \"\")\n",
    "\n",
    "    file_name = results[0]['file_name']\n",
    "        \n",
    "    return prompt_context, file_name\n",
    "\n",
    "def create_prompt(user_question):\n",
    "    \"\"\"\n",
    "    Create a prompt for the language model by combining the user question with context retrieved\n",
    "    from the cortex search service and chat history (if enabled). Format the prompt according to\n",
    "    the expected input format of the model.\n",
    "\n",
    "    Args:\n",
    "        user_question (str): The user's question to generate a prompt for.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated prompt for the language model.\n",
    "    \"\"\"\n",
    "\n",
    "    chat_history = get_chat_history()\n",
    "    if chat_history != []:\n",
    "        question_summary = make_chat_history_summary(chat_history, user_question)\n",
    "        prompt_context, file_name  = cortex_search(question_summary)\n",
    "    else:\n",
    "        prompt_context, file_name = cortex_search(user_question)\n",
    "        question_summary = ''\n",
    "\n",
    "    prompt = f\"\"\"You are a documentation specialist focused on providing precise answers based on provided documentation. \n",
    "\n",
    "        Input Context:\n",
    "        Context: {prompt_context}\n",
    "        Question: {question_summary}\n",
    "        Chat History: {chat_history}\n",
    "        \n",
    "        Instructions:\n",
    "        1. Analyze the provided context carefully\n",
    "        2. Frame responses to build upon any relevant chat history\n",
    "        3. Structure answers as follows:\n",
    "           - Direct answer to the question\n",
    "           - Required prerequisites or dependencies\n",
    "           - Step-by-step implementation (if applicable)\n",
    "           - Important limitations or warnings\n",
    "        \n",
    "        If information is not found in context:\n",
    "        1. Explicitly state what information is missing\n",
    "        2. Avoid assumptions or external references\n",
    "        3. Specify what additional context would help answer the question\n",
    "        \n",
    "        Remember: Only reference information from the provided context.\n",
    "        \n",
    "        Response:\"\"\"\n",
    "    return prompt, file_name\n",
    "\n",
    "def display_response(my_question):\n",
    "    with st.status(\"In progress...\") as status:\n",
    "        # Get the response from the AI model\n",
    "        response, name = complete(model_name, my_question)\n",
    "        \n",
    "        # Display the response from the model\n",
    "        st.markdown(response)\n",
    "        status.update(label=\"Done!\", state=\"complete\", expanded=True)\n",
    "        \n",
    "        # Display the source document name\n",
    "        with st.container():\n",
    "            display_name = f\"This information came from {name}\"\n",
    "            st.markdown(f\"This information came from {name}\")\n",
    "\n",
    "# Main code\n",
    "def main():\n",
    "    st.title(f\":speech_balloon: Chatbot with Snowflake Cortex with Anthropic Claude\")\n",
    "\n",
    "    init_config_options()\n",
    "    init_messages()\n",
    "\n",
    "    icons = {\"assistant\": \"❄️\", \"user\": \"👤\"}\n",
    "    \n",
    "    if question := st.chat_input(\"Ask a question...\"):\n",
    "        # Add user message to chat history\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
    "        # Display user message in chat message container\n",
    "        with st.chat_message(\"user\", avatar=icons[\"user\"]):\n",
    "            st.markdown(question.replace(\"$\", \"\\$\"))\n",
    "\n",
    "        # Display assistant response in chat message container\n",
    "        with st.chat_message(\"assistant\", avatar=icons[\"assistant\"]):\n",
    "            message_placeholder = st.empty()\n",
    "            # question = question.replace(\"'\", \"\")\n",
    "            with st.spinner(\"Thinking...\"):\n",
    "                # Generate the response\n",
    "                prompt, file_name = create_prompt(question)\n",
    "                generated_response = complete(model_name, prompt)\n",
    "                \n",
    "                # Store the generated response directly in session state\n",
    "                st.session_state.gen_response = generated_response\n",
    "                \n",
    "                # Display the generated response\n",
    "                message_placeholder.markdown(generated_response)\n",
    "\n",
    "        st.session_state.messages.append(\n",
    "            {\"role\": \"assistant\", \"content\": generated_response}\n",
    "        )\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    session = get_active_session()\n",
    "    main()\n"
   ]
  }
 ]
}